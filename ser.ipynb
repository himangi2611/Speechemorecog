{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfL3wEhGUGUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'speech-emotion-recog:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4885269%2F8236396%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240430%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240430T205015Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2aa515a04b8ae9a9dd2d1f6653a855cdcd7b6381603a873711da210808e59bd5153ad69148f8665e2ccfc2a59a3c5a499ef56698fccf1be18311ca5d261330c95b26b573c1e8e6b460f9538b79e8038343d997d34894ff0fc416f2090fd17d39c3fa3d81899b30d76a9cd7f694792bfb751d8a82beb5220671a13948109a5b256320cbbafa0cbedef56429fc66e39a56186aad9bbf26e5eb8e04bb2009e803f44e1211718281ee87e335b5e13470dbdf2ebdb1ebb0abf5a8b9209c5bc907198a2c968737212c4a564cd66ea39df5b24769fafb042f173db3c179fd39912d7e2bdff1b924cd66e4ceec2b5def5f08ea096693614848921b9e1da55d8bb584c08e'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNax102KUGUS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Audio\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs6yqUscUGUS"
      },
      "outputs": [],
      "source": [
        "Ravdess = \"/kaggle/input/speech-emotion-recog/RAVDESS/audio_speech_actors_01-24/\"\n",
        "Crema = \"/kaggle/input/speech-emotion-recog/CREMA-D/AudioWAV/\"\n",
        "Tess = \"/kaggle/input/speech-emotion-recog/TESS/TESS/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
        "Savee = \"/kaggle/input/speech-emotion-recog/SAVEE/ALL/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55uQJ8-7UGUS"
      },
      "outputs": [],
      "source": [
        "\n",
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in ravdess_directory_list:\n",
        "    actor = os.listdir(Ravdess + dir)\n",
        "    for file in actor:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('-')\n",
        "        if len(part) >= 3:\n",
        "            emotion_code = int(part[2])\n",
        "            if emotion_code == 1:\n",
        "                emotion = 'neutral'\n",
        "            elif emotion_code == 2:\n",
        "                emotion = 'calm'\n",
        "            elif emotion_code == 3:\n",
        "                emotion = 'happy'\n",
        "            elif emotion_code == 4:\n",
        "                emotion = 'sad'\n",
        "            elif emotion_code == 5:\n",
        "                emotion = 'angry'\n",
        "            elif emotion_code == 6:\n",
        "                emotion = 'fear'\n",
        "            elif emotion_code == 7:\n",
        "                emotion = 'disgust'\n",
        "            elif emotion_code == 8:\n",
        "                emotion = 'surprise'\n",
        "            else:\n",
        "                emotion = 'unknown'\n",
        "            file_emotion.append(emotion)\n",
        "        else:\n",
        "            file_emotion.append('unknown')\n",
        "        file_path.append(os.path.join(Ravdess, dir, file))\n",
        "\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "\n",
        "Ravdess_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpc52ycUGUS"
      },
      "outputs": [],
      "source": [
        "crema_directory_list = os.listdir(Crema)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in crema_directory_list:\n",
        "    file_path.append(Crema + file)\n",
        "    part=file.split('_')\n",
        "    if part[2] == 'SAD':\n",
        "        file_emotion.append('sad')\n",
        "    elif part[2] == 'ANG':\n",
        "        file_emotion.append('angry')\n",
        "    elif part[2] == 'DIS':\n",
        "        file_emotion.append('disgust')\n",
        "    elif part[2] == 'FEA':\n",
        "        file_emotion.append('fear')\n",
        "    elif part[2] == 'HAP':\n",
        "        file_emotion.append('happy')\n",
        "    elif part[2] == 'NEU':\n",
        "        file_emotion.append('neutral')\n",
        "    else:\n",
        "        file_emotion.append('Unknown')\n",
        "\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Crema_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkvqsZ1YUGUS"
      },
      "outputs": [],
      "source": [
        "tess_directory_list = os.listdir(Tess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in tess_directory_list:\n",
        "    directories = os.listdir(os.path.join(Tess, dir))\n",
        "    for file in directories:\n",
        "        part = file.split('.')[0]\n",
        "        split_parts = part.split('_')\n",
        "        if len(split_parts) > 2:\n",
        "            part = split_parts[2]\n",
        "            if part == 'ps':\n",
        "                file_emotion.append('surprise')\n",
        "            else:\n",
        "                file_emotion.append(part)\n",
        "            file_path.append(os.path.join(Tess, dir, file))\n",
        "\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Tess_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-gKowVjUGUS"
      },
      "outputs": [],
      "source": [
        "savee_directory_list = os.listdir(Savee)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in savee_directory_list:\n",
        "    file_path.append(Savee + file)\n",
        "    part = file.split('_')[1]\n",
        "    ele = part[:-6]\n",
        "    if ele=='a':\n",
        "        file_emotion.append('angry')\n",
        "    elif ele=='d':\n",
        "        file_emotion.append('disgust')\n",
        "    elif ele=='f':\n",
        "        file_emotion.append('fear')\n",
        "    elif ele=='h':\n",
        "        file_emotion.append('happy')\n",
        "    elif ele=='n':\n",
        "        file_emotion.append('neutral')\n",
        "    elif ele=='sa':\n",
        "        file_emotion.append('sad')\n",
        "    else:\n",
        "        file_emotion.append('surprise')\n",
        "\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Savee_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Z_dWw-UGUS"
      },
      "outputs": [],
      "source": [
        "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
        "data_path.to_csv(\"data_path.csv\",index=False)\n",
        "data_path.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rseLV9KZUGUT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.title('Count of Emotions', size=16)\n",
        "sns.countplot(data=data_path, x='Emotions')\n",
        "plt.ylabel('Count', size=12)\n",
        "plt.xlabel('Emotions', size=12)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grv-p8boUGUT"
      },
      "outputs": [],
      "source": [
        "def create_waveplot(data, sr, e):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.waveshow(data, sr=sr)\n",
        "    plt.show()\n",
        "\n",
        "def create_spectrogram(data, sr, e):\n",
        "    X = librosa.stft(data)\n",
        "    Xdb = librosa.amplitude_to_db(abs(X))\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "    plt.colorbar()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1anyT3uUGUT"
      },
      "outputs": [],
      "source": [
        "emotion='sad'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rek7wwbLMHLS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOoHOv7kUGUT"
      },
      "outputs": [],
      "source": [
        "emotion='angry'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff4sLCgRUGUT"
      },
      "outputs": [],
      "source": [
        "emotion='fear'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz7WByX_UGUT"
      },
      "outputs": [],
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate=rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
        "\n",
        "\n",
        "path = np.array(data_path.Path)[1]\n",
        "data, sample_rate = librosa.load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl6RUCb1UGUT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "Audio(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU26tgPqUGUT"
      },
      "outputs": [],
      "source": [
        "x = noise(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVVN-U9iUGUT"
      },
      "outputs": [],
      "source": [
        "x = stretch(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6vbZq8cUGUU"
      },
      "outputs": [],
      "source": [
        "x = shift(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPl2Hl5lUGUU"
      },
      "outputs": [],
      "source": [
        "x = pitch(data, sample_rate)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEcL_f7aUGUU"
      },
      "outputs": [],
      "source": [
        "def extract_features(data):\n",
        "    result = np.array([])\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "    result=np.hstack((result, zcr))\n",
        "    stft = np.abs(librosa.stft(data))\n",
        "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, chroma_stft))\n",
        "\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mfcc))\n",
        "\n",
        "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "    result = np.hstack((result, rms))\n",
        "\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mel))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJZXUwmpUGUU"
      },
      "outputs": [],
      "source": [
        "def get_features(path):\n",
        "\n",
        "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "\n",
        "\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2))\n",
        "\n",
        "    new_data = stretch(data)\n",
        "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "    res3 = extract_features(data_stretch_pitch)\n",
        "    result = np.vstack((result, res3))\n",
        "\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trQQrE2NUGUU"
      },
      "outputs": [],
      "source": [
        "X, Y = [], []  # Define X and Y lists before the loop\n",
        "X_batch, Y_batch = [], []\n",
        "batch_size = 1000  # Adjust batch size as needed\n",
        "\n",
        "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
        "    feature = get_features(path)\n",
        "    for ele in feature:\n",
        "        X_batch.append(ele)\n",
        "        Y_batch.append(emotion)\n",
        "\n",
        "    # Append batch to main lists after processing a chunk\n",
        "    if len(X_batch) >= batch_size:\n",
        "        X.extend(X_batch)\n",
        "        Y.extend(Y_batch)\n",
        "        X_batch.clear()\n",
        "        Y_batch.clear()\n",
        "\n",
        "        print(f\"Processed {batch_size} samples. Total samples processed: {len(X)}\")\n",
        "\n",
        "# Append remaining batch (if any)\n",
        "X.extend(X_batch)\n",
        "Y.extend(Y_batch)\n",
        "\n",
        "print(f\"Finished processing. Total samples processed: {len(X)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz4w_CWuUGUV"
      },
      "outputs": [],
      "source": [
        "len(X), len(Y), data_path.Path.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvrSCSU2UGUV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aD6SEiYUGUV"
      },
      "outputs": [],
      "source": [
        "Features = pd.DataFrame(X)\n",
        "Features['labels'] = Y\n",
        "Features.to_csv('features.csv', index=False)\n",
        "Features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29S0dWBZUGUV"
      },
      "outputs": [],
      "source": [
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGqXmWx4UGUV"
      },
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ki_oNDpUGUV"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FupOu4ZHUGUV"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc_UfLGIUGUV"
      },
      "outputs": [],
      "source": [
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnSe9xlpUGUV"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=8, activation='softmax'))\n",
        "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHZ1obc6UGUV"
      },
      "outputs": [],
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
        "history=model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), callbacks=[rlrp])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K23rF3LtUGUV"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "epochs = [i for i in range(100)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "test_acc = history.history['val_accuracy']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "fig.set_size_inches(20,6)\n",
        "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
        "ax[0].set_title('Training & Testing Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
        "ax[1].set_title('Training & Testing Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g67H2mXyUGUV"
      },
      "outputs": [],
      "source": [
        "pred_test = model.predict(x_test)\n",
        "y_pred = encoder.inverse_transform(pred_test)\n",
        "\n",
        "y_test = encoder.inverse_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZYX9zhkUGUW"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
        "df['Predicted Labels'] = y_pred.flatten()\n",
        "df['Actual Labels'] = y_test.flatten()\n",
        "\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnWAu35UGUW"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (12, 10))\n",
        "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
        "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
        "plt.title('Confusion Matrix', size=20)\n",
        "plt.xlabel('Predicted Labels', size=14)\n",
        "plt.ylabel('Actual Labels', size=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LKNVXt_UGUW"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISxgIWk-UGUW"
      },
      "outputs": [],
      "source": [
        "model.save('ser (1).h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRQcNqWJUGUW",
        "outputId": "12bdae61-231f-4658-c10b-83079109200d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.125.64.194\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.64.194:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.381s\n",
            "your url is: https://tangy-shoes-drop.loca.lt\n",
            "2024-05-29 08:19:20.738501: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-29 08:19:20.738579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-29 08:19:20.742601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-29 08:19:20.763008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-29 08:19:22.171943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2/2 [==============================] - 0s 17ms/step\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "! streamlit run app_ser.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QNlc-J3UGUa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4885269,
          "sourceId": 8236396,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}